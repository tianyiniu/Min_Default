{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Logistic Regression\"\"\"\n",
    "\n",
    "from re import sub\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read features.txt and initialize feature dictionaries. Adapated from Brandon's LSTM code.\"\"\"\n",
    "\n",
    "def get_strings(data_file):\n",
    "  \"\"\"Process input file into a list of strings.\"\"\"\n",
    "  input_file = open(data_file)\n",
    "  input_file.readline() # Skip first line\n",
    "  UR_strings, SR_strings, syll_lengths = [], [], []\n",
    "  ur_num = 0\n",
    "\n",
    "  for line in input_file.readlines():\n",
    "    columns = line.rstrip().split(\",\")\n",
    "    if len(columns) == 2:\n",
    "      ur, sr = columns\n",
    "      if sr == \"\" or ur == \"\":\n",
    "        continue\n",
    "      ur_num += 1\n",
    "\n",
    "      syll_lengths.append(len([seg for seg in ur.split(\" \") if seg != \"\"]))\n",
    "      UR_strings.append(ur)\n",
    "      SR_strings.append(sr[-5:]) # Last 5 characters correspond to plural suffix\n",
    "    else:\n",
    "       print(line)\n",
    "       raise Exception(\"Training data error! All lines should have 2 columns in TD files!\")\n",
    "  input_file.close()\n",
    "\n",
    "  return UR_strings, SR_strings, syll_lengths\n",
    "\n",
    "def get_arrays(UR_strings, SR_strings, syll_lengths, symbol2feats, suffix2label, override_max_syll=0):\n",
    "  if override_max_syll:\n",
    "    assert override_max_syll > max(syll_lengths)\n",
    "    max_len = override_max_syll\n",
    "  else: \n",
    "    max_len = max(syll_lengths)\n",
    "  \n",
    "  X_list = []\n",
    "  Y_list = []\n",
    "  padding_strs = []\n",
    "  for word_index, syll_length in enumerate(syll_lengths):\n",
    "    padding = \" \".join([\"_\"]*(max_len-syll_length))\n",
    "    this_ur = UR_strings[word_index]+\" \"+padding # Singular form + padding as string\n",
    "    padding_strs.append(this_ur)\n",
    "    this_sr = SR_strings[word_index][-5:] # Suffix as string\n",
    "\n",
    "    #Fix some errors in data files:\n",
    "    this_ur = sub(\" J \", \" Y \", this_ur)\n",
    "    this_ur = sub(\" C \", \" CH \", this_ur)\n",
    "\n",
    "    X_list.append([symbol2feats[seg] for seg in this_ur.split(\" \") if seg != \"\"])\n",
    "    Y_list.append(suffix2label[this_sr])\n",
    "\n",
    "  X = np.array(X_list)\n",
    "  Y = np.array(Y_list)\n",
    "\n",
    "  return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implements a pooling function to pool together phonetic feature vectors from different segments into a representative embedding vector.\"\"\"\n",
    "\n",
    "# Pooling function \n",
    "\n",
    "def pool_average(X):\n",
    "    \"\"\"Pools phonetic feature vectors by averaging across all segments.\"\"\"\n",
    "    # X.shape (n, 5, 19)\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "def pool_sum(X):\n",
    "    \"\"\"Pools phonetic feature vectors by summation across all segments.\"\"\"\n",
    "    return np.sum(X, axis=1)\n",
    "\n",
    "def pool_last(X):\n",
    "    \"\"\"Pools phonetic feature vectors by only returning the final segment.\"\"\"\n",
    "    last = X[:, -1, :]\n",
    "    return last\n",
    "\n",
    "def pool_concat(X):\n",
    "\t\"\"\"Pools features by concat each features vector head-to-tail. Results in a word-level feature vector of 5x19\"\"\"\n",
    "\treturn np.array([np.concatenate(submatrices, axis=0) for submatrices in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        68\n",
      "           1       1.00      1.00      1.00        88\n",
      "           2       1.00      1.00      1.00        82\n",
      "\n",
      "    accuracy                           1.00       238\n",
      "   macro avg       1.00      1.00      1.00       238\n",
      "weighted avg       1.00      1.00      1.00       238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load training data and fit logistic regression model\"\"\"\n",
    "\n",
    "FEATURES_FILE = \"featsNew\"\n",
    "TRAINING_DATA = \"./EqualDefault/equalFreq_train.txt\"\n",
    "POOLING_FUNC = pool_concat # TODO change pooling function\n",
    "\n",
    "#inputs\n",
    "feat_file = open(FEATURES_FILE, \"r\")\n",
    "feat_names = feat_file.readline().rstrip().split(\"\\t\")[1:] # Skip first space\n",
    "symbol2feats = {'_': [0.0 for f in feat_names]}\n",
    "\n",
    "for line in feat_file.readlines():\n",
    "  columns = line.rstrip().split(\"\\t\")\n",
    "  seg = columns[0]\n",
    "  values = [{\"-\":-1.0, \"+\":1.0, \"0\":0.0}[v] for v in columns[1:]]\n",
    "  symbol2feats[seg] = values\n",
    "\n",
    "#outputs\n",
    "suffix2label = {\n",
    "\t\"Y IY0\": 0, #yee\n",
    "\t\"W AH0\": 1, #wuh\n",
    "\t\"L EY0\": 2 #lay\n",
    "\t}\n",
    "\n",
    "URs, SRs, Ls = get_strings(TRAINING_DATA)\n",
    "X, y = get_arrays(URs, SRs, Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X = POOLING_FUNC(X) \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model on validation split\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "           1       1.00      0.99      0.99        96\n",
      "           2       0.99      1.00      0.99        96\n",
      "\n",
      "    accuracy                           1.00       288\n",
      "   macro avg       1.00      1.00      1.00       288\n",
      "weighted avg       1.00      1.00      1.00       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test set\"\"\"\n",
    "TEST_DATA = \"./EqualDefault/equalFreq_test.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        32\n",
      "           1       1.00      0.94      0.97        32\n",
      "           2       1.00      1.00      1.00        32\n",
      "\n",
      "    accuracy                           0.98        96\n",
      "   macro avg       0.98      0.98      0.98        96\n",
      "weighted avg       0.98      0.98      0.98        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test mutant set\"\"\"\n",
    "TEST_DATA = \"./EqualDefault/equalFreq_test_Mutants.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        96\n",
      "           1       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98        96\n",
      "   macro avg       0.50      0.49      0.49        96\n",
      "weighted avg       1.00      0.98      0.99        96\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test liquids set\"\"\"\n",
    "TEST_DATA = \"./EqualDefault/equalFreq_test_L.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75        60\n",
      "           1       1.00      0.45      0.62        40\n",
      "           2       0.91      0.50      0.65        40\n",
      "\n",
      "    accuracy                           0.70       140\n",
      "   macro avg       0.84      0.65      0.67       140\n",
      "weighted avg       0.80      0.70      0.68       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test new templates set\"\"\"\n",
    "TEST_DATA = \"./EqualDefault/equalFreq_testNewTemplates.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label, override_max_syll=5)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      40.0\n",
      "           2       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00      40.0\n",
      "   macro avg       0.00      0.00      0.00      40.0\n",
      "weighted avg       0.00      0.00      0.00      40.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test H set\"\"\"\n",
    "TEST_DATA = \"./EqualDefault/equalFreq_test_H.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
