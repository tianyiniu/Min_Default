{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use a traditional neural network setup to classify plural class.\"\"\"\n",
    "\n",
    "from re import sub\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read features.txt and initialize feature dictionaries. Adapated from Brandon's LSTM code.\"\"\"\n",
    "\n",
    "def get_strings(data_file):\n",
    "  \"\"\"Process input file into a list of strings.\"\"\"\n",
    "  input_file = open(data_file)\n",
    "  input_file.readline() # Skip first line\n",
    "  UR_strings, SR_strings, syll_lengths = [], [], []\n",
    "  ur_num = 0\n",
    "\n",
    "  for line in input_file.readlines():\n",
    "    columns = line.rstrip().split(\",\")\n",
    "    if len(columns) == 2:\n",
    "      ur, sr = columns\n",
    "      if sr == \"\" or ur == \"\":\n",
    "        continue\n",
    "      ur_num += 1\n",
    "\n",
    "      syll_lengths.append(len([seg for seg in ur.split(\" \") if seg != \"\"]))\n",
    "      UR_strings.append(ur)\n",
    "      SR_strings.append(sr[-5:]) # Last 5 characters correspond to plural suffix\n",
    "    else:\n",
    "       print(line)\n",
    "       raise Exception(\"Training data error! All lines should have 2 columns in TD files!\")\n",
    "  input_file.close()\n",
    "\n",
    "  return UR_strings, SR_strings, syll_lengths\n",
    "\n",
    "def get_arrays(UR_strings, SR_strings, syll_lengths, symbol2feats, suffix2label, override_max_syll=0):\n",
    "  if override_max_syll:\n",
    "    assert override_max_syll > max(syll_lengths)\n",
    "    max_len = override_max_syll\n",
    "  else: \n",
    "    max_len = max(syll_lengths)\n",
    "  \n",
    "  X_list = []\n",
    "  Y_list = []\n",
    "  padding_strs = []\n",
    "  for word_index, syll_length in enumerate(syll_lengths):\n",
    "    padding = \" \".join([\"_\"]*(max_len-syll_length))\n",
    "    this_ur = UR_strings[word_index]+\" \"+padding # Singular form + padding as string\n",
    "    padding_strs.append(this_ur)\n",
    "    this_sr = SR_strings[word_index][-5:] # Suffix as string\n",
    "\n",
    "    #Fix some errors in data files:\n",
    "    this_ur = sub(\" J \", \" Y \", this_ur)\n",
    "    this_ur = sub(\" C \", \" CH \", this_ur)\n",
    "\n",
    "    X_list.append([symbol2feats[seg] for seg in this_ur.split(\" \") if seg != \"\"])\n",
    "    Y_list.append(suffix2label[this_sr])\n",
    "\n",
    "  X = np.array(X_list)\n",
    "  Y = np.array(Y_list)\n",
    "\n",
    "  return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implements a pooling function to pool together phonetic feature vectors from different segments into a representative embedding vector.\"\"\"\n",
    "\n",
    "# Pooling function \n",
    "\n",
    "def pool_average(X):\n",
    "    \"\"\"Pools phonetic feature vectors by averaging across all segments.\"\"\"\n",
    "    # X.shape (n, 5, 19)\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "def pool_sum(X):\n",
    "    \"\"\"Pools phonetic feature vectors by summation across all segments.\"\"\"\n",
    "    return np.sum(X, axis=1)\n",
    "\n",
    "def pool_last(X):\n",
    "    \"\"\"Pools phonetic feature vectors by only returning the final segment.\"\"\"\n",
    "    last = X[:, -1, :]\n",
    "    return last\n",
    "\n",
    "def pool_concat(X):\n",
    "\t\"\"\"Pools features by concat each features vector head-to-tail. Results in a word-level feature vector of 5x19\"\"\"\n",
    "\treturn np.array([np.concatenate(submatrices, axis=0) for submatrices in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defines FFN model\"\"\"\n",
    "\n",
    "# Define the FFN model\n",
    "class SimpleFFN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleFFN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1188, 5, 19)\n",
      "(1188, 95)\n",
      "95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianyiniu/miniconda3/envs/mlenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0372\n",
      "Epoch [2/10], Loss: 0.0307\n",
      "Epoch [3/10], Loss: 0.0025\n",
      "Epoch [4/10], Loss: 0.0008\n",
      "Epoch [5/10], Loss: 0.0011\n",
      "Epoch [6/10], Loss: 0.0004\n",
      "Epoch [7/10], Loss: 0.0003\n",
      "Epoch [8/10], Loss: 0.0005\n",
      "Epoch [9/10], Loss: 0.0004\n",
      "Epoch [10/10], Loss: 0.0004\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load training data and fit logistic regression model\"\"\"\n",
    "\n",
    "FEATURES_FILE = \"feats.txt\"\n",
    "TRAINING_DATA = \"./Equal_FrequencyCondition/equalFreq_train.txt\"\n",
    "POOLING_FUNC = pool_concat # TODO change pooling function\n",
    "\n",
    "#inputs\n",
    "feat_file = open(FEATURES_FILE, \"r\")\n",
    "feat_names = feat_file.readline().rstrip().split(\"\\t\")[1:] # Skip first space\n",
    "symbol2feats = {'_': [0.0 for f in feat_names]}\n",
    "\n",
    "for line in feat_file.readlines():\n",
    "  columns = line.rstrip().split(\"\\t\")\n",
    "  seg = columns[0]\n",
    "  values = [{\"-\":-1.0, \"+\":1.0, \"0\":0.0}[v] for v in columns[1:]]\n",
    "  symbol2feats[seg] = values\n",
    "\n",
    "#outputs\n",
    "suffix2label = {\n",
    "\t\"Y IY0\": 0, #yee\n",
    "\t\"W AH0\": 1, #wuh\n",
    "\t\"L EY0\": 2 #lay\n",
    "\t}\n",
    "\n",
    "URs, SRs, Ls = get_strings(TRAINING_DATA)\n",
    "X, y = get_arrays(URs, SRs, Ls, symbol2feats, suffix2label)\n",
    "\n",
    "print(X.shape)\n",
    "X = POOLING_FUNC(X)\n",
    "print(X.shape)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = one_hot(torch.tensor(y_train).to(torch.int64), num_classes=3).to(torch.float32)\n",
    "\n",
    "\n",
    "# Prepare data loaders\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[-1] # TODO make variable\n",
    "\n",
    "print(input_dim)\n",
    "hidden_dim = 100\n",
    "output_dim = 3\n",
    "\n",
    "model = SimpleFFN(input_dim, hidden_dim, output_dim)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        84\n",
      "           1       1.00      1.00      1.00        84\n",
      "           2       1.00      1.00      1.00        70\n",
      "\n",
      "    accuracy                           1.00       238\n",
      "   macro avg       1.00      1.00      1.00       238\n",
      "weighted avg       1.00      1.00      1.00       238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the inference function\n",
    "def inference(model, inputs):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No need to compute gradients for inference\n",
    "        outputs = model(inputs)\n",
    "        predictions = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities\n",
    "        predictions_index = torch.argmax(predictions, dim=1)\n",
    "    return predictions_index\n",
    "\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
    "\n",
    "# Perform inference\n",
    "y_pred = inference(model, X_valid)\n",
    "class_report = classification_report(y_valid, y_pred)\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "           1       1.00      1.00      1.00        96\n",
      "           2       1.00      1.00      1.00        96\n",
      "\n",
      "    accuracy                           1.00       288\n",
      "   macro avg       1.00      1.00      1.00       288\n",
      "weighted avg       1.00      1.00      1.00       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test set\"\"\"\n",
    "TEST_DATA = \"./Equal_FrequencyCondition/equalFreq_test.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_test_one_hot = one_hot(torch.tensor(y_test).to(torch.int64), num_classes=3).to(torch.float32)\n",
    "\n",
    "y_pred = inference(model, X_test)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        30\n",
      "           1       0.97      1.00      0.98        30\n",
      "           2       1.00      0.97      0.98        30\n",
      "\n",
      "    accuracy                           0.99        90\n",
      "   macro avg       0.99      0.99      0.99        90\n",
      "weighted avg       0.99      0.99      0.99        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test mutant set\"\"\"\n",
    "TEST_DATA = \"./Equal_FrequencyCondition/equalFreq_test_Mutants.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_test_one_hot = one_hot(torch.tensor(y_test).to(torch.int64), num_classes=3).to(torch.float32)\n",
    "\n",
    "y_pred = inference(model, X_test)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9270833333333334\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test liquids set\"\"\"\n",
    "TEST_DATA = \"./Equal_FrequencyCondition/equalFreq_test_L.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_test_one_hot = one_hot(torch.tensor(y_test).to(torch.int64), num_classes=3).to(torch.float32)\n",
    "\n",
    "y_pred = inference(model, X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)\n",
    "# Can't use classification matrix because only one gold class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "torch.Size([120, 95])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.97      0.67        40\n",
      "           1       0.95      0.50      0.66        40\n",
      "           2       0.95      0.53      0.68        40\n",
      "\n",
      "    accuracy                           0.67       120\n",
      "   macro avg       0.80      0.67      0.67       120\n",
      "weighted avg       0.80      0.67      0.67       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test new templates set\"\"\"\n",
    "TEST_DATA = \"./Equal_FrequencyCondition/equalFreq_testNewTemplates.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Need to pad X_test\n",
    "padded_X_test = []\n",
    "for dp in X_test:\n",
    "    padding_length = input_dim - len(dp)\n",
    "    padding = [0.0]*padding_length\n",
    "    padded_dp = dp.tolist() + padding\n",
    "    padded_X_test.append(padded_dp)\n",
    "\n",
    "X_test = torch.tensor(padded_X_test, dtype=torch.float32)\n",
    "y_test_one_hot = one_hot(torch.tensor(y_test).to(torch.int64), num_classes=3).to(torch.float32)\n",
    "\n",
    "y_pred = inference(model, X_test)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
