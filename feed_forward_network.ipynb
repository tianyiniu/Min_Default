{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Use a traditional neural network setup to classify plural class.\"\"\"\n",
    "\n",
    "from re import sub\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read features.txt and initialize feature dictionaries. Adapated from Brandon's LSTM code.\"\"\"\n",
    "\n",
    "def get_strings(data_file):\n",
    "  \"\"\"Process input file into a list of strings.\"\"\"\n",
    "  input_file = open(data_file)\n",
    "  input_file.readline() # Skip first line\n",
    "  UR_strings, SR_strings, syll_lengths = [], [], []\n",
    "  ur_num = 0\n",
    "\n",
    "  for line in input_file.readlines():\n",
    "    columns = line.rstrip().split(\",\")\n",
    "    if len(columns) == 2:\n",
    "      ur, sr = columns\n",
    "      if sr == \"\" or ur == \"\":\n",
    "        continue\n",
    "      ur_num += 1\n",
    "\n",
    "      syll_lengths.append(len([seg for seg in ur.split(\" \") if seg != \"\"]))\n",
    "      UR_strings.append(ur)\n",
    "      SR_strings.append(sr[-5:]) # Last 5 characters correspond to plural suffix\n",
    "    else:\n",
    "       print(line)\n",
    "       raise Exception(\"Training data error! All lines should have 2 columns in TD files!\")\n",
    "  input_file.close()\n",
    "\n",
    "  return UR_strings, SR_strings, syll_lengths\n",
    "\n",
    "def get_arrays(UR_strings, SR_strings, syll_lengths, symbol2feats, suffix2label, override_max_syll=0):\n",
    "  if override_max_syll:\n",
    "    assert override_max_syll > max(syll_lengths)\n",
    "    max_len = override_max_syll\n",
    "  else: \n",
    "    max_len = max(syll_lengths)\n",
    "  \n",
    "  X_list = []\n",
    "  Y_list = []\n",
    "  padding_strs = []\n",
    "  for word_index, syll_length in enumerate(syll_lengths):\n",
    "    padding = \" \".join([\"_\"]*(max_len-syll_length))\n",
    "    this_ur = UR_strings[word_index]+\" \"+padding # Singular form + padding as string\n",
    "    padding_strs.append(this_ur)\n",
    "    this_sr = SR_strings[word_index][-5:] # Suffix as string\n",
    "\n",
    "    #Fix some errors in data files:\n",
    "    this_ur = sub(\" J \", \" Y \", this_ur)\n",
    "    this_ur = sub(\" C \", \" CH \", this_ur)\n",
    "\n",
    "    X_list.append([symbol2feats[seg] for seg in this_ur.split(\" \") if seg != \"\"])\n",
    "    Y_list.append(suffix2label[this_sr])\n",
    "\n",
    "  X = np.array(X_list)\n",
    "  Y = np.array(Y_list)\n",
    "\n",
    "  return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implements a pooling function to pool together phonetic feature vectors from different segments into a representative embedding vector.\"\"\"\n",
    "\n",
    "# Pooling function \n",
    "\n",
    "def pool_average(X):\n",
    "    \"\"\"Pools phonetic feature vectors by averaging across all segments.\"\"\"\n",
    "    # X.shape (n, 5, 19)\n",
    "    return np.mean(X, axis=1)\n",
    "\n",
    "def pool_sum(X):\n",
    "    \"\"\"Pools phonetic feature vectors by summation across all segments.\"\"\"\n",
    "    return np.sum(X, axis=1)\n",
    "\n",
    "def pool_last(X):\n",
    "    \"\"\"Pools phonetic feature vectors by only returning the final segment.\"\"\"\n",
    "    last = X[:, -1, :]\n",
    "    return last\n",
    "\n",
    "def pool_concat(X):\n",
    "\t\"\"\"Pools features by concat each features vector head-to-tail. Results in a word-level feature vector of 5x19\"\"\"\n",
    "\treturn np.array([np.concatenate(submatrices, axis=0) for submatrices in X])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Defines FFN model\"\"\"\n",
    "\n",
    "# Define the FFN model\n",
    "class SimpleFFN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleFFN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1188, 5, 19)\n",
      "(1188, 95)\n",
      "95\n",
      "Epoch [1/10], Loss: 0.0795\n",
      "Epoch [2/10], Loss: 0.0070\n",
      "Epoch [3/10], Loss: 0.0024\n",
      "Epoch [4/10], Loss: 0.0020\n",
      "Epoch [5/10], Loss: 0.0003\n",
      "Epoch [6/10], Loss: 0.0006\n",
      "Epoch [7/10], Loss: 0.0006\n",
      "Epoch [8/10], Loss: 0.0005\n",
      "Epoch [9/10], Loss: 0.0002\n",
      "Epoch [10/10], Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load training data and fit logistic regression model\"\"\"\n",
    "\n",
    "FEATURES_FILE = \"feats.txt\"\n",
    "TRAINING_DATA = \"./Equal_FrequencyCondition/equalFreq_train.txt\"\n",
    "POOLING_FUNC = pool_concat # TODO change pooling function\n",
    "\n",
    "#inputs\n",
    "feat_file = open(FEATURES_FILE, \"r\")\n",
    "feat_names = feat_file.readline().rstrip().split(\"\\t\")[1:] # Skip first space\n",
    "symbol2feats = {'_': [0.0 for f in feat_names]}\n",
    "\n",
    "for line in feat_file.readlines():\n",
    "  columns = line.rstrip().split(\"\\t\")\n",
    "  seg = columns[0]\n",
    "  values = [{\"-\":-1.0, \"+\":1.0, \"0\":0.0}[v] for v in columns[1:]]\n",
    "  symbol2feats[seg] = values\n",
    "\n",
    "#outputs\n",
    "suffix2label = {\n",
    "\t\"Y IY0\": 0, #yee\n",
    "\t\"W AH0\": 1, #wuh\n",
    "\t\"L EY0\": 2 #lay\n",
    "\t}\n",
    "\n",
    "URs, SRs, Ls = get_strings(TRAINING_DATA)\n",
    "X, y = get_arrays(URs, SRs, Ls, symbol2feats, suffix2label)\n",
    "\n",
    "print(X.shape)\n",
    "X = POOLING_FUNC(X)\n",
    "print(X.shape)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = one_hot(torch.tensor(y_train).to(torch.int64), num_classes=3).to(torch.float32)\n",
    "\n",
    "\n",
    "# Prepare data loaders\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[-1] # TODO make variable\n",
    "\n",
    "print(input_dim)\n",
    "hidden_dim = 100\n",
    "output_dim = 3\n",
    "\n",
    "model = SimpleFFN(input_dim, hidden_dim, output_dim)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        84\n",
      "           1       1.00      1.00      1.00        84\n",
      "           2       1.00      1.00      1.00        70\n",
      "\n",
      "    accuracy                           1.00       238\n",
      "   macro avg       1.00      1.00      1.00       238\n",
      "weighted avg       1.00      1.00      1.00       238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the inference function\n",
    "def inference(model, inputs):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():  # No need to compute gradients for inference\n",
    "        outputs = model(inputs)\n",
    "        predictions = torch.sigmoid(outputs)  # Apply sigmoid to get probabilities\n",
    "        predictions_index = torch.argmax(predictions, dim=1)\n",
    "    return predictions_index\n",
    "\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
    "\n",
    "# Perform inference\n",
    "y_pred = inference(model, X_valid)\n",
    "class_report = classification_report(y_valid, y_pred)\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "           1       1.00      1.00      1.00        96\n",
      "           2       1.00      1.00      1.00        96\n",
      "\n",
      "    accuracy                           1.00       288\n",
      "   macro avg       1.00      1.00      1.00       288\n",
      "weighted avg       1.00      1.00      1.00       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test set\"\"\"\n",
    "TEST_DATA = \"./Equal_FrequencyCondition/equalFreq_test.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_test_one_hot = one_hot(torch.tensor(y_test).to(torch.int64), num_classes=3).to(torch.float32)\n",
    "\n",
    "y_pred = inference(model, X_test)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        30\n",
      "           1       0.97      1.00      0.98        30\n",
      "           2       1.00      0.97      0.98        30\n",
      "\n",
      "    accuracy                           0.99        90\n",
      "   macro avg       0.99      0.99      0.99        90\n",
      "weighted avg       0.99      0.99      0.99        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test mutant set\"\"\"\n",
    "TEST_DATA = \"./Equal_FrequencyCondition/equalFreq_test_Mutants.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_test_one_hot = one_hot(torch.tensor(y_test).to(torch.int64), num_classes=3).to(torch.float32)\n",
    "\n",
    "y_pred = inference(model, X_test)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "\n",
      "    accuracy                           1.00        96\n",
      "   macro avg       1.00      1.00      1.00        96\n",
      "weighted avg       1.00      1.00      1.00        96\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test liquids set\"\"\"\n",
    "TEST_DATA = \"./Equal_FrequencyCondition/equalFreq_test_L.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_test_one_hot = one_hot(torch.tensor(y_test).to(torch.int64), num_classes=3).to(torch.float32)\n",
    "\n",
    "y_pred = inference(model, X_test)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (120x76 and 95x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[114], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m X_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_test, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     10\u001b[0m y_test_one_hot \u001b[38;5;241m=\u001b[39m one_hot(torch\u001b[38;5;241m.\u001b[39mtensor(y_test)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 12\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m class_report \u001b[38;5;241m=\u001b[39m classification_report(y_test, y_pred)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(class_report)\n",
      "Cell \u001b[1;32mIn[110], line 5\u001b[0m, in \u001b[0;36minference\u001b[1;34m(model, inputs)\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No need to compute gradients for inference\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(outputs)  \u001b[38;5;66;03m# Apply sigmoid to get probabilities\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     predictions_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(predictions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[108], line 12\u001b[0m, in \u001b[0;36mSimpleFFN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 12\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(out)\n",
      "File \u001b[1;32mc:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tniu\\AppData\\Local\\miniconda3\\envs\\llm-base\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (120x76 and 95x100)"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate test new templates set\"\"\"\n",
    "TEST_DATA = \"./Equal_FrequencyCondition/equalFreq_testNewTemplates.txt\"\n",
    "\n",
    "test_SGs, test_PLs, test_Ls = get_strings(TEST_DATA)\n",
    "X_test, y_test = get_arrays(test_SGs, test_PLs, test_Ls, symbol2feats, suffix2label)\n",
    "\n",
    "X_test = POOLING_FUNC(X_test) # TODO Pool feature vectors\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_test_one_hot = one_hot(torch.tensor(y_test).to(torch.int64), num_classes=3).to(torch.float32)\n",
    "\n",
    "y_pred = inference(model, X_test)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
