{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simple scaled-down Transformer model implementation\"\"\"\n",
    "\n",
    "from re import sub\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from matplotlib.pyplot import plot, legend, show, xlabel, ylabel, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training dataset and dataloader\n",
    "class PhoneticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, phonetic_features, labels):\n",
    "        self.phonetic_features = phonetic_features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.phonetic_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'phonetic_features': torch.tensor(self.phonetic_features[idx], dtype=torch.int32),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read features.txt and initialize feature dictionaries. Adapated from Brandon's LSTM code.\"\"\"\n",
    "\n",
    "def get_strings(data_file):\n",
    "  \"\"\"Process input file into a list of strings.\"\"\"\n",
    "  input_file = open(data_file)\n",
    "  input_file.readline() # Skip first line\n",
    "  UR_strings, SR_strings, syll_lengths = [], [], []\n",
    "  ur_num = 0\n",
    "\n",
    "  for line in input_file.readlines():\n",
    "    columns = line.rstrip().split(\",\")\n",
    "    if len(columns) == 2:\n",
    "      ur, sr = columns\n",
    "      if sr == \"\" or ur == \"\":\n",
    "        continue\n",
    "      ur_num += 1\n",
    "\n",
    "      syll_lengths.append(len([seg for seg in ur.split(\" \") if seg != \"\"]))\n",
    "      UR_strings.append(ur)\n",
    "      SR_strings.append(sr[-5:]) # Last 5 characters correspond to plural suffix\n",
    "    else:\n",
    "       print(line)\n",
    "       raise Exception(\"Training data error! All lines should have 2 columns in TD files!\")\n",
    "  input_file.close()\n",
    "\n",
    "  return UR_strings, SR_strings, syll_lengths\n",
    "\n",
    "def get_data(UR_strings, SR_strings, syll_lengths, symbol2idx, suffix2label, override_max_syll=0):\n",
    "  \"\"\"\n",
    "  UR_strings[:2] = ['IY0 G UW1 CH', 'L OW1 K']\n",
    "  SR_strings[:2] = ['W AH0', 'L EY0']\n",
    "  syll_lengths[:2] = [4, 3]\n",
    "   \n",
    "  \"\"\"\n",
    "  if override_max_syll:\n",
    "    assert override_max_syll > max(syll_lengths)\n",
    "    max_len = override_max_syll\n",
    "  else: \n",
    "    max_len = max(syll_lengths)\n",
    "  \n",
    "  X_list = []\n",
    "  Y_list = []\n",
    "  padding_strs = []\n",
    "  for word_index, syll_length in enumerate(syll_lengths):\n",
    "    padding = \" \".join([\"_\"]*(max_len-syll_length))\n",
    "    this_ur = UR_strings[word_index]+\" \"+padding # Singular form + padding as string\n",
    "    padding_strs.append(this_ur)\n",
    "    this_sr = SR_strings[word_index][-5:] # Suffix as string\n",
    "\n",
    "    #Fix some errors in data files:\n",
    "    this_ur = sub(\" J \", \" Y \", this_ur)\n",
    "    this_ur = sub(\" C \", \" CH \", this_ur)\n",
    "\n",
    "    X_list.append([symbol2idx[seg] for seg in this_ur.split(\" \") if seg != \"\"])\n",
    "    Y_list.append(suffix2label[this_sr])\n",
    "\n",
    "  X = np.array(X_list)\n",
    "  Y = np.array(Y_list)\n",
    "\n",
    "  return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_FILE = \"feats.txt\"\n",
    "TRAINING_DATA = \"./EqualDefault/equalFreq_train.txt\"\n",
    "\n",
    "# Create phonetic features embedding matrix\n",
    "feat_file = open(FEATURES_FILE, \"r\")\n",
    "feat_names = feat_file.readline().rstrip().split(\"\\t\")[1:] # Skip first space\n",
    "symbol2feats = {'_': [0.0 for f in feat_names]}\n",
    "\n",
    "symbol2idx = {}\n",
    "curr_idx = 0\n",
    "\n",
    "for line in feat_file.readlines():\n",
    "  columns = line.rstrip().split(\"\\t\")\n",
    "  seg = columns[0]\n",
    "  values = [{\"-\":-1.0, \"+\":1.0, \"0\":0.0}[v] for v in columns[1:]]\n",
    "  symbol2feats[seg] = values\n",
    "  symbol2idx[seg] = curr_idx\n",
    "  curr_idx += 1\n",
    "\n",
    "symbol2idx[\"_\"] = curr_idx\n",
    "idx2symbol = {idx:symbol for symbol, idx in symbol2idx.items()}\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = [symbol2feats[idx2symbol[i]] for i in range(len(idx2symbol))]\n",
    "\n",
    "#outputs\n",
    "suffix2label = {\n",
    "\t\"Y IY0\": 0, #yee\n",
    "\t\"W AH0\": 1, #wuh\n",
    "\t\"L EY0\": 2 #lay\n",
    "\t}\n",
    "\n",
    "URs, SRs, Ls = get_strings(TRAINING_DATA)\n",
    "X, y = get_data(URs, SRs, Ls, symbol2idx, suffix2label)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train).to(device)\n",
    "y_train_one_hot = one_hot(torch.tensor(y_train).to(torch.int64), num_classes=3).to(torch.float32).to(device)\n",
    "\n",
    "# Assuming phonetic_features and labels are available as lists or numpy arrays\n",
    "dataset = PhoneticDataset(X_train, y_train_one_hot)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 10108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_385326/74298541.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'phonetic_features': torch.tensor(self.phonetic_features[idx], dtype=torch.int32),\n",
      "/tmp/ipykernel_385326/74298541.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.7043711543083191\n",
      "Epoch 100, Loss: 0.6622956395149231\n",
      "Epoch 150, Loss: 0.6491360664367676\n",
      "Epoch 200, Loss: 0.6659570336341858\n",
      "Epoch 250, Loss: 0.6279313564300537\n",
      "Epoch 300, Loss: 0.6379976868629456\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_385326/2950623062.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodified_gpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphonetic_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Modify the input layer to accept phonetic feature vectors\n",
    "class GPT2WithPhoneticFeatures(torch.nn.Module):\n",
    "\tdef __init__(self, gpt2_model, feature_embeddings):\n",
    "\t\tsuper(GPT2WithPhoneticFeatures, self).__init__()\n",
    "\t\tself.gpt2_model = gpt2_model\n",
    "\t\tnum_features, embedding_dim = feature_embeddings.shape # (43, 19)\n",
    "\t\tself.phonetic_embedding = nn.Embedding.from_pretrained(feature_embeddings, freeze=True)\n",
    "\t\t\n",
    "\t\tassert embedding_dim == config.n_embd\n",
    "\t\t\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.classifier = nn.Linear(config.n_embd, 3)\n",
    "\n",
    "\tdef forward(self, phonetic_inputs):\n",
    "\t\tembd_inputs = self.phonetic_embedding(phonetic_inputs)\n",
    "\t\toutputs = self.gpt2_model(inputs_embeds=embd_inputs)\n",
    "\t\trelued = self.relu(outputs.last_hidden_state)\n",
    "\t\tlogits = self.classifier(relued)\n",
    "\t\treturn logits\n",
    "\n",
    "# Define the configuration\n",
    "config = GPT2Config(\n",
    "\tvocab_size=43, \n",
    "\tn_positions=5,\n",
    "\tn_ctx=5,\n",
    "\tn_embd=19,\n",
    "\tn_layer=2,\n",
    "\tn_head=1\n",
    ")\n",
    "\n",
    "# Load the pre-trained GPT model\n",
    "model = GPT2Model(config)\n",
    "\n",
    "epochs = 1000\n",
    "# Initialize embedding matrix\n",
    "embeddings = torch.tensor(embedding_matrix, dtype=torch.float32).to(device) # Shape (43, 19)\n",
    "\n",
    "# Initialize the modified GPT-2 model\n",
    "modified_gpt2 = GPT2WithPhoneticFeatures(model, embeddings).to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {pytorch_total_params}\")\n",
    "\n",
    "# Define the optimizer and scheduler\n",
    "optimizer = AdamW(modified_gpt2.parameters(), lr=1e-4)\n",
    "criterion = CrossEntropyLoss()\n",
    "total_steps = len(dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training loop\n",
    "allLosses = []\n",
    "modified_gpt2.train()\n",
    "for epoch in range(epochs):\n",
    "\tfor batch in dataloader:\n",
    "\t\tphonetic_features = batch['phonetic_features']\n",
    "\n",
    "\t\tlabels = batch['labels']\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\toutputs = modified_gpt2(phonetic_features)\n",
    "\t\tloss = criterion(outputs, labels)\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\toptimizer.step()\n",
    "\t\tscheduler.step()\n",
    "\t\n",
    "\tif (epoch+1) % 50 == 0:\n",
    "\t\tprint(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\t\tallLosses.append(loss.item())\n",
    "\n",
    "# Save the trained model\n",
    "# torch.save(modified_gpt2.state_dict(), 'modified_gpt2_with_phonetic_features.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x77ba649f49a0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot average loss (for checking lr, etc.)\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(allLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       328\n",
      "           1       0.00      0.00      0.00       308\n",
      "           2       0.33      1.00      0.50       314\n",
      "\n",
      "    accuracy                           0.33       950\n",
      "   macro avg       0.11      0.33      0.17       950\n",
      "weighted avg       0.11      0.33      0.16       950\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X_valid = torch.tensor(X_train, dtype=torch.int32)\n",
    "\n",
    "# Inference and accuracy measurement\n",
    "modified_gpt2.eval()\n",
    "with torch.no_grad():\n",
    "\ttest_outputs = modified_gpt2(X_valid)\n",
    "\ttest_outputs_last_layer = test_outputs[:, -1, :]\n",
    "\tpredicted_classes = torch.argmax(test_outputs_last_layer, dim=-1)\n",
    "\n",
    "class_report = classification_report(y_train, predicted_classes)\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
